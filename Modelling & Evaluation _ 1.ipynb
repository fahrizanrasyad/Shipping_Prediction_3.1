{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n",
    "    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n",
    "    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n",
    "    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n",
    "    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n",
    "    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) \n",
    "    print(\"AUC: %.2f\" % auc(fpr, tpr))\n",
    "    print(confusion_matrix(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('Train.csv')\n",
    "\n",
    "X = df2.drop(columns=['ID', 'Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender', 'Reached.on.Time_Y.N'])\n",
    "y = df2['Reached.on.Time_Y.N'] # target\n",
    "\n",
    "# split Train and Test\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.63\n",
      "Precision (Test Set): 0.71\n",
      "Recall (Test Set): 0.67\n",
      "F1-Score (Test Set): 0.69\n",
      "AUC: 0.63\n",
      "[[ 771  541]\n",
      " [ 665 1323]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr1 = LogisticRegression(random_state=42)\n",
    "lr1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr1.predict(X_test)\n",
    "eval_classification(lr1, y_pred, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6359267437329523\n",
      "Test score:0.6345454545454545\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ' + str(lr1.score(X_train, y_train))) \n",
    "print('Test score:' + str(lr1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.63\n",
      "Precision (Test Set): 0.71\n",
      "Recall (Test Set): 0.66\n",
      "F1-Score (Test Set): 0.68\n",
      "AUC: 0.62\n",
      "[[ 766  546]\n",
      " [ 671 1317]]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# list hyperparameter:\n",
    "penalty = ['l2','l1','elasticnet']\n",
    "C = [0.0001, 0.001, 0.002] \n",
    "hyperparameters = dict(penalty=penalty, C=C)\n",
    "\n",
    "lr2 = LogisticRegression(random_state=42) \n",
    "model1 = RandomizedSearchCV(lr2, hyperparameters, cv=5, random_state=42, scoring='precision'\n",
    "                            ,n_iter=30)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_test)\n",
    "eval_classification(model1, y_pred, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7073227073227073\n",
      "Test score:0.7069243156199678\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ' + str(model1.score(X_train, y_train))) \n",
    "print('Test score:' + str(model1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.65\n",
      "Precision (Test Set): 0.73\n",
      "Recall (Test Set): 0.68\n",
      "F1-Score (Test Set): 0.70\n",
      "AUC: 0.64\n",
      "[[ 804  508]\n",
      " [ 644 1344]]\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn1.predict(X_test)\n",
    "eval_classification(knn1, y_pred, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7821795038316665\n",
      "Test score:0.6509090909090909\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ' + str(knn1.score(X_train, y_train))) \n",
    "print('Test score:' + str(knn1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.67\n",
      "Precision (Test Set): 0.79\n",
      "Recall (Test Set): 0.61\n",
      "F1-Score (Test Set): 0.69\n",
      "AUC: 0.68\n",
      "[[ 989  323]\n",
      " [ 776 1212]]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "# list hyperparameter:\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "weights = ['uniform','distance']\n",
    "hyperparameters = dict(n_neighbors=n_neighbors, p=p, algorithm=algorithm, weights=weights)\n",
    "\n",
    "knn2 = KNeighborsClassifier()\n",
    "model2 = RandomizedSearchCV(knn2, hyperparameters, cv=5, random_state=42, scoring='precision')\n",
    "\n",
    "# Fit Model & Evaluasi\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred = model2.predict(X_test)\n",
    "eval_classification(model2, y_pred, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8708381171067738\n",
      "Test score:0.7895765472312704\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ' + str(model2.score(X_train, y_train))) \n",
    "print('Test score:' + str(model2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.66\n",
      "Precision (Test Set): 0.72\n",
      "Recall (Test Set): 0.73\n",
      "F1-Score (Test Set): 0.72\n",
      "AUC: 0.64\n",
      "[[ 736  576]\n",
      " [ 541 1447]]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt1 = DecisionTreeClassifier(random_state=42)\n",
    "dt1.fit(X_train,y_train)\n",
    "\n",
    "y_pred = dt1.predict(X_test)\n",
    "eval_classification(dt1, y_pred, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score:0.6615151515151515\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ' + str(dt1.score(X_train, y_train))) \n",
    "print('Test score:' + str(dt1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Set): 0.67\n",
      "Precision (Test Set): 0.75\n",
      "Recall (Test Set): 0.67\n",
      "F1-Score (Test Set): 0.71\n",
      "AUC: 0.67\n",
      "[[ 872  440]\n",
      " [ 659 1329]]\n",
      "Train score: 0.7587648014859532\n",
      "Test score:0.7074793718392334\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "# list hyperparameter: # parameter ini masih bisa dicoba2\n",
    "max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] \n",
    "min_samples_split = [2, 5, 10, 100] \n",
    "min_samples_leaf = [1, 2, 4, 10, 20, 50]\n",
    "max_features = ['auto', 'sqrt'] \n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split, \n",
    "                       min_samples_leaf=min_samples_leaf,\n",
    "                       max_features=max_features\n",
    "                      )\n",
    "\n",
    "# Inisialisasi Model\n",
    "dt2 = DecisionTreeClassifier(random_state=42)\n",
    "model3 = RandomizedSearchCV(dt2, hyperparameters, cv=5, random_state=42, scoring='f1'\n",
    "                           , n_iter=30) # cobain lagi n_iter\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluation\n",
    "y_pred = model3.predict(X_test)# Cek performa dari model\n",
    "eval_classification(model3, y_pred, X_train, y_train, X_test, y_test)\n",
    "print('Train score: ' + str(model3.score(X_train, y_train))) \n",
    "print('Test score:' + str(model3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train score: ' + str(model3.score(X_train, y_train))) \n",
    "print('Test score:' + str(model3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "tree.plot_tree(model3.best_estimator_,\n",
    "               feature_names = X.columns.tolist(), \n",
    "               class_names=['0','1'],\n",
    "               filled = True, max_depth=5, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model3.best_estimator_.feature_importances_, index=X.columns)\n",
    "ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('feature')\n",
    "plt.title('feature importance score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
